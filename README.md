# TASK-1

*COMPANY*: CODTECH IT SOLUTIONS

*NAME*: SONIYA D

*INTERN ID*: CT04DR2644

*DOMAIN*: DATA SCIENCE

*DURATION*: 4 WEEKS

*MENTOR*: NEELA SANTHOSH

##In Task–1 of the CodTech Data Science Internship, the main objective was to design and implement a data pipeline that automates the process of data handling using Python. This task focused on understanding and applying the ETL process, which stands for Extract, Transform, and Load. Data pipelines play a crucial role in data science projects because they ensure that raw data is cleaned, processed, and made ready for analysis or modeling in an efficient and structured manner.
The first stage of the task involved data extraction. In this stage, data was collected and loaded into the system using the Pandas library. Pandas provides powerful data structures such as DataFrames that allow easy handling of tabular data. The dataset used in this task contained both numerical and categorical values along with missing entries, which closely resembles real-world datasets. Extracting data using Pandas helped in understanding how raw data is ingested into a data science workflow.
The second stage was data transformation and preprocessing, which is one of the most important steps in any data pipeline. Real-world data is often incomplete, inconsistent, and unstructured. To handle this, several preprocessing techniques were applied. Missing values in numerical columns were identified and handled using appropriate strategies such as replacing them with the mean value. Categorical variables were converted into numerical form using encoding techniques, which are necessary because machine learning models cannot directly work with text-based categories. Additionally, numerical features were scaled to ensure that all features are on a similar scale, improving data consistency and readiness for further analysis. These transformations were implemented using Scikit-learn, which provides reliable preprocessing tools widely used in the data science industry.
The final stage of the pipeline was data loading. After preprocessing, the transformed and cleaned data was saved into a new file in CSV format. This step represents the loading phase of the ETL process, where processed data is stored for future use, such as machine learning model training, reporting, or visualization. Automating this step ensures that whenever new data is provided, the same pipeline can be executed without manual intervention, saving time and reducing errors.
One of the key learning outcomes of this task was understanding how different components of a data pipeline work together as a single automated system. Instead of manually cleaning and processing data every time, the pipeline allows these steps to be executed sequentially through a single Python script. This approach improves efficiency, reproducibility, and scalability, which are essential qualities in professional data science workflows.
Through this task, I gained hands-on experience with essential Python libraries such as Pandas and Scikit-learn and developed a deeper understanding of data preprocessing techniques. I also learned how automation in data pipelines helps maintain data quality and consistency across projects. Overall, Task–1 provided a strong foundation in data engineering concepts and highlighted the importance of clean and well-structured data in successful data science projects.

#OUTPUT

<img width="485" height="328" alt="Image" src="https://github.com/user-attachments/assets/f83c6c0f-2111-490e-8f60-7516adc65b62" />
